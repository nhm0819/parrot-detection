{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"parrot.ipynb","provenance":[],"collapsed_sections":["oIyFSfpGbsJh"],"mount_file_id":"1QxGjFbrYN4ncrbTh-fPAAI94qMGSSKCm","authorship_tag":"ABX9TyPmB1KcShjWZVOChq11k15h"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"a49LOnovjFMH"},"source":["### Data Load"]},{"cell_type":"code","metadata":{"id":"d5Gc1CX93ykq","executionInfo":{"status":"ok","timestamp":1623743815096,"user_tz":-540,"elapsed":4943,"user":{"displayName":"노홍민","photoUrl":"","userId":"17395953209726241917"}}},"source":["!cp drive/MyDrive/KSA_AI/DeepLearning/Object_Detection/train.csv ./\n","!cp drive/MyDrive/KSA_AI/DeepLearning/Object_Detection/validation.csv ./\n","!cp drive/MyDrive/KSA_AI/DeepLearning/Object_Detection/test.csv ./\n","!cp drive/MyDrive/KSA_AI/DeepLearning/Object_Detection/SSD.zip ./"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"wlfyKSVI3zGB"},"source":["!unzip SSD.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2kJBfjkqs8mw","executionInfo":{"status":"ok","timestamp":1623743825456,"user_tz":-540,"elapsed":416,"user":{"displayName":"노홍민","photoUrl":"","userId":"17395953209726241917"}}},"source":["!mv '모듈8데이터(SSD_앵무새)' dataset"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"UysCU560ieG_","executionInfo":{"status":"ok","timestamp":1623743825457,"user_tz":-540,"elapsed":12,"user":{"displayName":"노홍민","photoUrl":"","userId":"17395953209726241917"}}},"source":["import os\n","dataset_dir = os.path.join(os.getcwd(), \"dataset\")\n","dataset_classes = os.listdir(dataset_dir)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bs3mV8Xds8fr","executionInfo":{"status":"ok","timestamp":1623743825458,"user_tz":-540,"elapsed":11,"user":{"displayName":"노홍민","photoUrl":"","userId":"17395953209726241917"}}},"source":["import shutil\n","for cls in dataset_classes:\n","    images = os.listdir(os.path.join(dataset_dir, cls, 'img'))\n","    for img in images:\n","        path = os.path.join(dataset_dir, cls, 'img', img)\n","        shutil.move(path, os.path.join(dataset_dir, cls))\n","    shutil.rmtree(os.path.join(dataset_dir, cls, 'img'))"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"P5Y0v8Gbs97n","colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"status":"ok","timestamp":1623743827005,"user_tz":-540,"elapsed":1556,"user":{"displayName":"노홍민","photoUrl":"","userId":"17395953209726241917"}},"outputId":"6090c21c-dfc0-42ba-97bd-8671ea8bb93e"},"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import os\n","import tqdm\n","from PIL import Image\n","\n","train_df = pd.read_csv('train.csv')\n","val_df = pd.read_csv('validation.csv')\n","test_df = pd.read_csv('test.csv')\n","train_df"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image_path</th>\n","      <th>xmin</th>\n","      <th>xmax</th>\n","      <th>ymin</th>\n","      <th>ymax</th>\n","      <th>class_id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>dataset/0001고핀/0001_00000116.jpg</td>\n","      <td>145</td>\n","      <td>214</td>\n","      <td>61</td>\n","      <td>154</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>dataset/0001고핀/0001_00000133.jpg</td>\n","      <td>199</td>\n","      <td>261</td>\n","      <td>87</td>\n","      <td>174</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>dataset/0001고핀/0001_00000073.jpg</td>\n","      <td>30</td>\n","      <td>97</td>\n","      <td>69</td>\n","      <td>158</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>dataset/0001고핀/0001_00000018.jpg</td>\n","      <td>131</td>\n","      <td>234</td>\n","      <td>14</td>\n","      <td>141</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>dataset/0001고핀/0001_00000099.jpg</td>\n","      <td>112</td>\n","      <td>202</td>\n","      <td>46</td>\n","      <td>178</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3035</th>\n","      <td>dataset/0011회색앵무/0011_00000252.jpg</td>\n","      <td>116</td>\n","      <td>199</td>\n","      <td>42</td>\n","      <td>120</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>3036</th>\n","      <td>dataset/0011회색앵무/0011_00000320.jpg</td>\n","      <td>108</td>\n","      <td>265</td>\n","      <td>8</td>\n","      <td>120</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>3037</th>\n","      <td>dataset/0011회색앵무/0011_00000199.jpg</td>\n","      <td>60</td>\n","      <td>208</td>\n","      <td>50</td>\n","      <td>211</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>3038</th>\n","      <td>dataset/0011회색앵무/0011_00000235.jpg</td>\n","      <td>14</td>\n","      <td>112</td>\n","      <td>14</td>\n","      <td>117</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>3039</th>\n","      <td>dataset/0011회색앵무/0011_00000247.jpg</td>\n","      <td>45</td>\n","      <td>120</td>\n","      <td>50</td>\n","      <td>125</td>\n","      <td>11</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3040 rows × 6 columns</p>\n","</div>"],"text/plain":["                              image_path  xmin  xmax  ymin  ymax  class_id\n","0       dataset/0001고핀/0001_00000116.jpg   145   214    61   154         1\n","1       dataset/0001고핀/0001_00000133.jpg   199   261    87   174         1\n","2       dataset/0001고핀/0001_00000073.jpg    30    97    69   158         1\n","3       dataset/0001고핀/0001_00000018.jpg   131   234    14   141         1\n","4       dataset/0001고핀/0001_00000099.jpg   112   202    46   178         1\n","...                                  ...   ...   ...   ...   ...       ...\n","3035  dataset/0011회색앵무/0011_00000252.jpg   116   199    42   120        11\n","3036  dataset/0011회색앵무/0011_00000320.jpg   108   265     8   120        11\n","3037  dataset/0011회색앵무/0011_00000199.jpg    60   208    50   211        11\n","3038  dataset/0011회색앵무/0011_00000235.jpg    14   112    14   117        11\n","3039  dataset/0011회색앵무/0011_00000247.jpg    45   120    50   125        11\n","\n","[3040 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"oIyFSfpGbsJh"},"source":["### TF Record"]},{"cell_type":"code","metadata":{"id":"pvKJ8LBXBe3L","executionInfo":{"status":"ok","timestamp":1623743827006,"user_tz":-540,"elapsed":10,"user":{"displayName":"노홍민","photoUrl":"","userId":"17395953209726241917"}}},"source":["def _bytes_feature(value, is_list=False):\n","    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n","    if isinstance(value, type(tf.constant(0))):\n","        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n","    \n","    if not is_list:\n","        value = [value]\n","    \n","    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n","\n","def _float_feature(value, is_list=False):\n","    \"\"\"Returns a float_list from a float / double.\"\"\"\n","        \n","    if not is_list:\n","        value = [value]\n","        \n","    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n","\n","def _int64_feature(value, is_list=False):\n","    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n","        \n","    if not is_list:\n","        value = [value]\n","        \n","    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n","\n","\n","def serialize(sensor_features):\n","    # Create a dictionary mapping the feature name to the \n","    # tf.Example-compatible data type.\n","    feature = {\n","        # 'image': _bytes_feature(tf.io.encode_jpeg(image), is_list=False)\n","        'image': _float_feature(sensor_features['image'], is_list=False),\n","        'class_id': _bytes_feature(sensor_features['class_id'], is_list=False),\n","        'xmin': _int64_feature(sensor_features['xmin'], is_list=False),\n","        'xmax': _int64_feature(sensor_features['xmax'], is_list=False),\n","        'ymin': _int64_feature(sensor_features['ymin'], is_list=False),\n","        'ymax': _int64_feature(sensor_features['ymax'], is_list=False)\n","        }\n","\n","    # Create a Features message using tf.train.Example.\n","    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n","    return example_proto.SerializeToString()"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"OeWc5kucBfjC","executionInfo":{"status":"ok","timestamp":1623743827007,"user_tz":-540,"elapsed":9,"user":{"displayName":"노홍민","photoUrl":"","userId":"17395953209726241917"}}},"source":["def serialize_example(feature0, feature1, feature2, feature3, feature4, feature5, feature6, feature7):\n","  feature = {\n","      'image': _bytes_feature(feature0),\n","      'image_name': _bytes_feature(feature1),\n","      'sex': _int64_feature(feature3),\n","      'age_approx': _int64_feature(feature4),\n","      'anatom_site_general_challenge': _int64_feature(feature5),\n","      'source': _int64_feature(feature6),\n","      'target': _int64_feature(feature7)\n","  }\n","  example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n","  return example_proto.SerializeToString()"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"tD17zzOGeFJG","executionInfo":{"status":"ok","timestamp":1623743827008,"user_tz":-540,"elapsed":9,"user":{"displayName":"노홍민","photoUrl":"","userId":"17395953209726241917"}}},"source":[""],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"49md6MwOOKHX"},"source":["### Preprocessing"]},{"cell_type":"code","metadata":{"id":"JVmnOJaKMfB1","executionInfo":{"status":"ok","timestamp":1623743827009,"user_tz":-540,"elapsed":10,"user":{"displayName":"노홍민","photoUrl":"","userId":"17395953209726241917"}}},"source":["import os\n","import tensorflow as tf\n","import pandas as pd\n","\n","def create_tf_dataset(df, is_test=False):\n","    ds = tf.data.Dataset.from_tensor_slices(df[\"image_path\"].values)\n","\n","    if is_test:\n","        return ds\n","    else:\n","        ds_bbox = tf.data.Dataset.from_tensor_slices(\n","            [tf.constant(x) for x in df[['xmin', 'xmax', 'ymin', 'ymax']].values])\n","        target_ds = tf.data.Dataset.from_tensor_slices(df['class_id'].values)\n","\n","        ds = tf.data.Dataset.zip((ds, ds_bbox, target_ds))\n","        return ds\n","\n","\n","def parse_image(filename, image_size, augmentation=False):\n","    # parts = tf.strings.split(filename, '/')\n","    # image_id = parts[-1]\n","    image = tf.io.read_file(filename)\n","    image = tf.image.decode_jpeg(image)\n","    # image = tf.image.convert_image_dtype(image, tf.float32)\n","    image = tf.image.resize(image, image_size,  method=tf.image.ResizeMethod.LANCZOS5, antialias=True)\n","    # image = tf.keras.preprocessing.image.img_to_array(image)\n","    return image\n","\n","\n","def prep_tf_dataset(img_path, coords, class_id, image_size, augmentation=False):\n","    img_tensor = parse_image(img_path, image_size=image_size)\n","    coords = tf.cast(coords, tf.float32)\n","    coords = coords/image_size[0]\n","    return img_tensor, coords, class_id\n","\n","def ds_generator(df_path, image_size=(300,300), augmentation=False):\n","    '''\n","    input : pd.DataFrame(columns=['image_name', 'xmin', 'xmax', 'ymin', 'ymax', 'class_id'])\n","    output : <BatchDataset shapes: ((batch, 300, 300, 3), (batch, 4), (batch,)), types: (tf.float32, tf.float32, tf.int64)>\n","    '''\n","    df = pd.read_csv(df_path)\n","    data_len = len(df)\n","    ds = create_tf_dataset(df)\n","    ds = ds.map(lambda x, y, z: (prep_tf_dataset(x, y, z, image_size=image_size, augmentation=augmentation)),\n","                num_parallel_calls=tf.data.experimental.AUTOTUNE).prefetch(tf.data.experimental.AUTOTUNE)\n","    # ds = ds.shuffle(3000).batch(batch_size)\n","    return ds, data_len"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FsaP1bpRjoxz"},"source":["### dataset generate"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fcMD-Cq9cGQo","executionInfo":{"status":"ok","timestamp":1623743832549,"user_tz":-540,"elapsed":5549,"user":{"displayName":"노홍민","photoUrl":"","userId":"17395953209726241917"}},"outputId":"b89981b7-1339-442e-e094-b2c56ff99844"},"source":["ds_generator('train.csv', image_size=(300, 300))"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<PrefetchDataset shapes: ((300, 300, None), (4,), ()), types: (tf.float32, tf.float32, tf.int64)>,\n"," 3040)"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"plUwYlr_jm7M","executionInfo":{"status":"ok","timestamp":1623743832550,"user_tz":-540,"elapsed":9,"user":{"displayName":"노홍민","photoUrl":"","userId":"17395953209726241917"}}},"source":["def get_data_shapes():\n","    \"\"\"Generating data shapes for tensorflow datasets.\n","    outputs:\n","        data shapes = output data shapes for (images, ground truth boxes, ground truth labels)\n","    \"\"\"\n","    return ([None, None, None], [None,], [])   ###\n"," \n","def get_padding_values():\n","    \"\"\"Generating padding values for missing values in batch for tensorflow datasets.\n","    outputs:\n","        padding values = padding values with dtypes for (images, ground truth boxes, ground truth labels)\n","    \"\"\"\n","    return (tf.constant(0, tf.float32), tf.constant(0, tf.float32), tf.constant(-1, tf.int64))"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"4Sglxh2AXsl8","executionInfo":{"status":"ok","timestamp":1623743832551,"user_tz":-540,"elapsed":9,"user":{"displayName":"노홍민","photoUrl":"","userId":"17395953209726241917"}}},"source":[""],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ay4hFVhvYIv2"},"source":["### bounding box calcuate func"]},{"cell_type":"code","metadata":{"id":"OcNBy8XYd7WJ","executionInfo":{"status":"ok","timestamp":1623743833047,"user_tz":-540,"elapsed":504,"user":{"displayName":"노홍민","photoUrl":"","userId":"17395953209726241917"}}},"source":["import tensorflow as tf\n","\n","def non_max_suppression(pred_bboxes, pred_labels, **kwargs):\n","    \"\"\"Applying non maximum suppression.\n","    Details could be found on tensorflow documentation.\n","    https://www.tensorflow.org/api_docs/python/tf/image/combined_non_max_suppression\n","    inputs:\n","        pred_bboxes = (batch_size, total_bboxes, total_labels, [x1, x2, y1, y2])\n","            total_labels should be 1 for binary operations like in rpn\n","        pred_labels = (batch_size, total_bboxes, total_labels)\n","        **kwargs = other parameters\n","    outputs:\n","        nms_boxes = (batch_size, max_detections, [x1, x2, y1, y2])\n","        nmsed_scores = (batch_size, max_detections)\n","        nmsed_classes = (batch_size, max_detections)\n","        valid_detections = (batch_size)\n","            Only the top valid_detections[i] entries in nms_boxes[i], nms_scores[i] and nms_class[i] are valid.\n","            The rest of the entries are zero paddings.\n","    \"\"\"\n","    return tf.image.combined_non_max_suppression(\n","        pred_bboxes,\n","        pred_labels,\n","        **kwargs\n","    )\n","\n","def generate_iou_map(bboxes, gt_boxes, transpose_perm=[0, 2, 1]):\n","    \"\"\"Calculating intersection over union values for each ground truth boxes in a dynamic manner.\n","    It is supported from 1d to 3d dimensions for bounding boxes.\n","    Even if bboxes have different rank from gt_boxes it should be work.\n","    inputs:\n","        bboxes = (dynamic_dimension, [x1, x2, y1, y2])\n","        gt_boxes = (dynamic_dimension, [x1, x2, y1, y2])\n","        transpose_perm = (transpose_perm_order)\n","            for 3d gt_boxes => [0, 2, 1]\n","    outputs:\n","        iou_map = (dynamic_dimension, total_gt_boxes)\n","            same rank with the gt_boxes\n","    \"\"\"\n","\n","    gt_rank = tf.rank(gt_boxes)\n","    gt_expand_axis = gt_rank - 2\n","    #\n","    bbox_x1, bbox_x2, bbox_y1, bbox_y2 = tf.split(bboxes, 4, axis=-1)\n","    gt_x1, gt_x2, gt_y1, gt_y2 = tf.split(gt_boxes, 4, axis=-1)\n","    # Calculate bbox and ground truth boxes areas\n","    gt_area = tf.squeeze((gt_y2 - gt_y1) * (gt_x2 - gt_x1), axis=-1)\n","    bbox_area = tf.squeeze((bbox_y2 - bbox_y1) * (bbox_x2 - bbox_x1), axis=-1)\n","    #\n","    # x_top = tf.maximum(bbox_x1, tf.transpose(gt_x1, transpose_perm))\n","    # y_top = tf.maximum(bbox_y1, tf.transpose(gt_y1, transpose_perm))\n","    # x_bottom = tf.minimum(bbox_x2, tf.transpose(gt_x2, transpose_perm))\n","    # y_bottom = tf.minimum(bbox_y2, tf.transpose(gt_y2, transpose_perm))\n","    x_top = tf.maximum(bbox_x1, tf.transpose(gt_x1))\n","    y_top = tf.maximum(bbox_y1, tf.transpose(gt_y1))\n","    x_bottom = tf.minimum(bbox_x2, tf.transpose(gt_x2))\n","    y_bottom = tf.minimum(bbox_y2, tf.transpose(gt_y2))\n","    ### Calculate intersection area\n","    intersection_area = tf.maximum(x_bottom - x_top, 0) * tf.maximum(y_bottom - y_top, 0)\n","    ### Calculate union area\n","    union_area = (tf.expand_dims(bbox_area, -1) + tf.expand_dims(gt_area, gt_expand_axis) - intersection_area)\n","    # Intersection over Union\n","    return intersection_area / union_area\n","\n","def get_bboxes_from_deltas(prior_boxes, deltas):\n","    \"\"\"Calculating bounding boxes for given bounding box and delta values.\n","    inputs:\n","        prior_boxes = (total_bboxes, [x1, x2, y1, y2])\n","        deltas = (batch_size, total_bboxes, [delta_y, delta_x, delta_h, delta_w])\n","    outputs:\n","        final_boxes = (batch_size, total_bboxes, [x1, x2, y1, y2])\n","    \"\"\"\n","    all_pbox_width = prior_boxes[..., 1] - prior_boxes[..., 0]\n","    all_pbox_height = prior_boxes[..., 3] - prior_boxes[..., 2]\n","    all_pbox_ctr_x = prior_boxes[..., 0] + 0.5 * all_pbox_width\n","    all_pbox_ctr_y = prior_boxes[..., 2] + 0.5 * all_pbox_height\n","    #\n","    all_bbox_width = tf.exp(deltas[..., 3]) * all_pbox_width\n","    all_bbox_height = tf.exp(deltas[..., 2]) * all_pbox_height\n","    all_bbox_ctr_x = (deltas[..., 1] * all_pbox_width) + all_pbox_ctr_x\n","    all_bbox_ctr_y = (deltas[..., 0] * all_pbox_height) + all_pbox_ctr_y\n","    #\n","    x1 = all_bbox_ctr_x - (0.5 * all_bbox_width)\n","    x2 = all_bbox_width + x1\n","    y1 = all_bbox_ctr_y - (0.5 * all_bbox_height)\n","    y2 = all_bbox_height + y1\n","    #\n","    return tf.stack([x1, x2, y1, y2], axis=-1)\n","\n","def get_deltas_from_bboxes(bboxes, gt_boxes):\n","    \"\"\"Calculating bounding box deltas for given bounding box and ground truth boxes.\n","    inputs:\n","        bboxes = (total_bboxes, [x1, x2, y1, y2])\n","        gt_boxes = (batch_size, total_bboxes, [x1, x2, y1, y2])\n","    outputs:\n","        final_deltas = (batch_size, total_bboxes, [delta_x, delta_y, delta_w, delta_h])\n","    \"\"\"\n","    bbox_width = bboxes[..., 1] - bboxes[..., 0]\n","    bbox_height = bboxes[..., 3] - bboxes[..., 2]\n","    bbox_ctr_x = bboxes[..., 0] + 0.5 * bbox_width\n","    bbox_ctr_y = bboxes[..., 2] + 0.5 * bbox_height\n","    #\n","    gt_width = gt_boxes[..., 1] - gt_boxes[..., 0]\n","    gt_height = gt_boxes[..., 3] - gt_boxes[..., 2]\n","    gt_ctr_x = gt_boxes[..., 0] + 0.5 * gt_width\n","    gt_ctr_y = gt_boxes[..., 2] + 0.5 * gt_height\n","    #\n","    bbox_width = tf.where(tf.equal(bbox_width, 0), 1e-3, bbox_width)\n","    bbox_height = tf.where(tf.equal(bbox_height, 0), 1e-3, bbox_height)\n","    delta_x = tf.where(tf.equal(gt_width, 0), tf.zeros_like(gt_width), tf.truediv((gt_ctr_x - bbox_ctr_x), bbox_width))\n","    delta_y = tf.where(tf.equal(gt_height, 0), tf.zeros_like(gt_height), tf.truediv((gt_ctr_y - bbox_ctr_y), bbox_height))\n","    delta_w = tf.where(tf.equal(gt_width, 0), tf.zeros_like(gt_width), tf.math.log(gt_width / bbox_width))\n","    delta_h = tf.where(tf.equal(gt_height, 0), tf.zeros_like(gt_height), tf.math.log(gt_height / bbox_height))\n","    #\n","    return tf.stack([delta_x, delta_y, delta_w, delta_h], axis=-1)\n","\n","def get_scale_for_nth_feature_map(k, m=6, scale_min=0.2, scale_max=0.9):\n","    \"\"\"Calculating scale value for nth feature map using the given method in the paper.\n","    inputs:\n","        k = nth feature map for scale calculation\n","        m = length of all using feature maps for detections, 6 for ssd300\n","    outputs:\n","        scale = calculated scale value for given index\n","    \"\"\"\n","    return scale_min + ((scale_max - scale_min) / (m - 1)) * (k - 1)\n","\n","def generate_base_prior_boxes(aspect_ratios, feature_map_index, total_feature_map):\n","    \"\"\"Generating top left prior boxes for given stride, height and width pairs of different aspect ratios.\n","    These prior boxes same with the anchors in Faster-RCNN.\n","    inputs:\n","        aspect_ratios = for all feature map shapes + 1 for ratio 1\n","        feature_map_index = nth feature maps for scale calculation\n","        total_feature_map = length of all using feature map for detections, 6 for ssd300\n","    outputs:\n","        base_prior_boxes = (prior_box_count, [x1, x2, y1, y2])\n","    \"\"\"\n","    current_scale = get_scale_for_nth_feature_map(feature_map_index, m=total_feature_map)\n","    next_scale = get_scale_for_nth_feature_map(feature_map_index + 1, m=total_feature_map)\n","    base_prior_boxes = []\n","    for aspect_ratio in aspect_ratios:\n","        height = current_scale / tf.sqrt(aspect_ratio)\n","        width = current_scale * tf.sqrt(aspect_ratio)\n","        base_prior_boxes.append([-width/2, width/2, -height/2, height/2])\n","    # 1 extra pair for ratio 1\n","    height = width = tf.sqrt(current_scale * next_scale)\n","    base_prior_boxes.append([-width/2, width/2, -height/2, height/2])\n","    return tf.cast(base_prior_boxes, dtype=tf.float32)\n","\n","def generate_prior_boxes(feature_map_shapes, aspect_ratios):\n","    \"\"\"Generating top left prior boxes for given stride, height and width pairs of different aspect ratios.\n","    These prior boxes same with the anchors in Faster-RCNN.\n","    inputs:\n","        feature_map_shapes = for all feature map output size\n","        aspect_ratios = for all feature map shapes + 1 for ratio 1\n","    outputs:\n","        prior_boxes = (total_prior_boxes, [y1, x1, y2, x2])\n","        -->prior_boxes = (total_prior_boxes, [x1, x2, y1, y2])\n","            these values in normalized format between [0, 1]\n","    \"\"\"\n","    prior_boxes = []\n","    for i, feature_map_shape in enumerate(feature_map_shapes):\n","        base_prior_boxes = generate_base_prior_boxes(aspect_ratios[i], i+1, len(feature_map_shapes))\n","        #\n","        stride = 1 / feature_map_shape\n","        grid_coords = tf.cast(tf.range(0, feature_map_shape) / feature_map_shape + stride / 2, dtype=tf.float32)\n","        grid_x, grid_y = tf.meshgrid(grid_coords, grid_coords)\n","        flat_grid_x, flat_grid_y = tf.reshape(grid_x, (-1, )), tf.reshape(grid_y, (-1, ))\n","        #\n","        grid_map = tf.stack([flat_grid_x, flat_grid_x, flat_grid_y, flat_grid_y], -1)\n","        #\n","        prior_boxes_for_feature_map = tf.reshape(base_prior_boxes, (1, -1, 4)) + tf.reshape(grid_map, (-1, 1, 4))\n","        prior_boxes_for_feature_map = tf.reshape(prior_boxes_for_feature_map, (-1, 4))\n","        #\n","        prior_boxes.append(prior_boxes_for_feature_map)\n","    prior_boxes = tf.concat(prior_boxes, axis=0)\n","    return tf.clip_by_value(prior_boxes, 0, 1)\n","\n","def renormalize_bboxes_with_min_max(bboxes, min_max):\n","    \"\"\"Renormalizing given bounding boxes to the new boundaries.\n","    r = (x - min) / (max - min)\n","    outputs:\n","        bboxes = (total_bboxes, [x1, x2, y1, y2])\n","        min_max = ([x_min, x_max, y_min, y_max])\n","    \"\"\"\n","    x_min, x_max, y_min, y_max = tf.split(min_max, 4)\n","    renomalized_bboxes = bboxes - tf.concat([x_min, x_max, y_min, y_max], -1)\n","    renomalized_bboxes /= tf.concat([x_max-x_min, x_max-x_min, y_max-y_min, y_max-y_min], -1)\n","    return tf.clip_by_value(renomalized_bboxes, 0, 1)\n","\n","def normalize_bboxes(bboxes, height, width):\n","    \"\"\"Normalizing bounding boxes.\n","    inputs:\n","        bboxes = (batch_size, total_bboxes, [x1, x2, y1, y2])\n","        height = image height\n","        width = image width\n","    outputs:\n","        normalized_bboxes = (batch_size, total_bboxes, [x1, x2, y1, y2])\n","            in normalized form [0, 1]\n","    \"\"\"\n","    x1 = bboxes[..., 0] / width\n","    x2 = bboxes[..., 1] / width\n","    y1 = bboxes[..., 2] / height\n","    y2 = bboxes[..., 2] / height\n","    return tf.stack([x1, x2, y1, y2], axis=-1)\n","\n","def denormalize_bboxes(bboxes, height, width):\n","    \"\"\"Denormalizing bounding boxes.\n","    inputs:\n","        bboxes = (batch_size, total_bboxes, [x1, x2, y1, y2])\n","            in normalized form [0, 1]\n","        height = image height\n","        width = image width\n","    outputs:\n","        denormalized_bboxes = (batch_size, total_bboxes, [x1, x2, y1, y2])\n","    \"\"\"\n","    x1 = bboxes[..., 0] / width\n","    x2 = bboxes[..., 1] / width\n","    y1 = bboxes[..., 2] / height\n","    y2 = bboxes[..., 2] / height\n","    return tf.round(tf.stack([x1, x2, y1, y2], axis=-1))"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"c1a5WejtYI0q","executionInfo":{"status":"ok","timestamp":1623743833047,"user_tz":-540,"elapsed":14,"user":{"displayName":"노홍민","photoUrl":"","userId":"17395953209726241917"}}},"source":["def generator(dataset, prior_boxes, hyper_params):\n","    \"\"\"Tensorflow data generator for fit method, yielding inputs and outputs.\n","    inputs:\n","        dataset = tf.data.Dataset, PaddedBatchDataset\n","        prior_boxes = (total_prior_boxes, [x1, x2, y1, y2])\n","            these values in normalized format between [0, 1]\n","        hyper_params = dictionary\n","    outputs:\n","        yield inputs, outputs\n","    \"\"\"\n","    while True:\n","        for image_data in dataset:\n","            img, gt_boxes, gt_labels = image_data\n","            actual_deltas, actual_labels = calculate_actual_outputs(prior_boxes, gt_boxes, gt_labels, hyper_params)\n","            yield img, (actual_deltas, actual_labels)\n","\n","# def calculate_actual_outputs(prior_boxes, gt_boxes, gt_labels, hyper_params):\n","#     \"\"\"Calculate ssd actual output values.\n","#     Batch operations supported.\n","#     inputs:\n","#         prior_boxes = (total_prior_boxes, [x1, x2, y1, y2])\n","#             these values in normalized format between [0, 1]\n","#         gt_boxes (batch_size, gt_box_size, [x1, x2, y1, y2])\n","#             these values in normalized format between [0, 1]\n","#         gt_labels (batch_size, gt_box_size)\n","#         hyper_params = dictionary\n","#     outputs:\n","#         bbox_deltas = (batch_size, total_bboxes, [delta_y, delta_x, delta_h, delta_w])\n","#         bbox_labels = (batch_size, total_bboxes, [0,0,...,0])\n","#     \"\"\"\n","#     batch_size = tf.shape(gt_boxes)[0]\n","#     total_labels = hyper_params[\"n_classes\"]\n","#     iou_threshold = hyper_params[\"iou_threshold\"]\n","#     variances = hyper_params[\"variances\"]\n","#     total_prior_boxes = prior_boxes.shape[0]\n","#     # Calculate iou values between each bboxes and ground truth boxes\n","#     iou_map = generate_iou_map(prior_boxes, gt_boxes)\n","#     # Get max index value for each row\n","#     max_indices_each_gt_box = tf.argmax(iou_map, axis=1, output_type=tf.int32)\n","#     # IoU map has iou values for every gt boxes and we merge these values column wise\n","#     merged_iou_map = tf.reduce_max(iou_map, axis=1)\n","#     #\n","#     pos_cond = tf.greater(merged_iou_map, iou_threshold)\n","#     #\n","#     gt_boxes_map = tf.gather(gt_boxes, max_indices_each_gt_box, batch_dims=0)\n","#     expanded_gt_boxes = tf.where(tf.expand_dims(pos_cond, -1), gt_boxes_map, tf.zeros_like(gt_boxes_map))\n","#     bbox_deltas = get_deltas_from_bboxes(prior_boxes, expanded_gt_boxes) / variances\n","#     #\n","#     gt_labels_map = tf.gather(gt_labels, max_indices_each_gt_box, batch_dims=0)\n","#     expanded_gt_labels = tf.where(pos_cond, gt_labels_map, tf.zeros_like(gt_labels_map))\n","#     bbox_labels = tf.one_hot(expanded_gt_labels, total_labels)\n","#     #\n","#     return bbox_deltas, bbox_labels"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"iUQ842rQKrWU","executionInfo":{"status":"ok","timestamp":1623743833048,"user_tz":-540,"elapsed":14,"user":{"displayName":"노홍민","photoUrl":"","userId":"17395953209726241917"}}},"source":["def calculate_actual_outputs(prior_boxes, gt_boxes, gt_labels, hyper_params):\n","    \"\"\"Calculate ssd actual output values.\n","    Batch operations supported.\n","    inputs:\n","        prior_boxes = (total_prior_boxes, [x1, x2, y1, y2])\n","            these values in normalized format between [0, 1]\n","        gt_boxes (batch_size, gt_box_size, [x1, x2, y1, y2])\n","            these values in normalized format between [0, 1]\n","        gt_labels (batch_size, gt_box_size)\n","        hyper_params = dictionary\n","    outputs:\n","        bbox_deltas = (batch_size, total_bboxes, [delta_y, delta_x, delta_h, delta_w])\n","        bbox_labels = (batch_size, total_bboxes, [0,0,...,0])\n","    \"\"\"\n","    batch_size = tf.shape(gt_boxes)[0]\n","    total_labels = hyper_params[\"n_classes\"]\n","    iou_threshold = hyper_params[\"iou_threshold\"]\n","    variances = hyper_params[\"variances\"]\n","    total_prior_boxes = prior_boxes.shape[0]\n","    # Calculate iou values between each bboxes and ground truth boxes\n","    iou_map = generate_iou_map(prior_boxes, gt_boxes)  # iou_map.shape : (8732,32)\n","    # Get max index value for each row\n","    max_indices_each_gt_box = tf.argmax(iou_map, axis=1, output_type=tf.int32)\n","    # IoU map has iou values for every gt boxes and we merge these values column wise\n","    merged_iou_map = tf.reduce_max(iou_map, axis=1)\n","    #\n","\n","    # pos_cond = tf.greater(merged_iou_map, iou_threshold)\n","    pos_cond = tf.greater(iou_map, iou_threshold)\n","\n","    #\n","    # gt_boxes_map = tf.gather(gt_boxes, max_indices_each_gt_box, batch_dims=1)\n","    # expanded_gt_boxes = tf.where(tf.expand_dims(pos_cond, -1), gt_boxes_map, tf.zeros_like(gt_boxes_map))\n","    # bbox_deltas = bbox_utils.get_deltas_from_bboxes(prior_boxes, expanded_gt_boxes) / variances\n","\n","    # gt_boxes_map = tf.gather(gt_boxes, max_indices_each_gt_box, batch_dims=1)\n","    expanded_gt_boxes = tf.where(tf.expand_dims(pos_cond, -1), gt_boxes, tf.zeros_like(gt_boxes))\n","    bbox_deltas = get_deltas_from_bboxes(prior_boxes, tf.transpose(expanded_gt_boxes, perm=[1,0,2]) ) / variances\n","\n","\n","    #\n","    # gt_labels_map = tf.gather(gt_labels, max_indices_each_gt_box, batch_dims=1)\n","    # expanded_gt_labels = tf.where(pos_cond, gt_labels_map, tf.zeros_like(gt_labels_map))\n","    # bbox_labels = tf.one_hot(expanded_gt_labels, total_labels)\n","    #\n","    #\n","    # gt_labels_map = tf.gather(gt_labels, max_indices_each_gt_box, batch_dims=1)\n","    expanded_gt_labels = tf.where(pos_cond, gt_labels, tf.zeros_like(gt_labels))\n","    bbox_labels = tf.one_hot(tf.transpose(expanded_gt_labels), total_labels)\n","    #\n","\n","    return bbox_deltas, bbox_labels"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YZ1xE1zGj225"},"source":["### Modeling"]},{"cell_type":"code","metadata":{"id":"c6o4AiZPrLxl","executionInfo":{"status":"ok","timestamp":1623743833049,"user_tz":-540,"elapsed":14,"user":{"displayName":"노홍민","photoUrl":"","userId":"17395953209726241917"}}},"source":["import tensorflow as tf\n","from tensorflow.keras.layers import Layer, Input, Conv2D, MaxPool2D, Activation\n","\n","class HeadWrapper(Layer):\n","    \"\"\"Merging all feature maps for detections.\n","    inputs:\n","        conv4_3 = (batch_size, (layer_shape x aspect_ratios), last_dimension)\n","            ssd300 conv4_3 shape => (38 x 38 x 4) = 5776\n","        conv7 = (batch_size, (layer_shape x aspect_ratios), last_dimension)\n","            ssd300 conv7 shape => (19 x 19 x 6) = 2166\n","        conv8_2 = (batch_size, (layer_shape x aspect_ratios), last_dimension)\n","            ssd300 conv8_2 shape => (10 x 10 x 6) = 600\n","        conv9_2 = (batch_size, (layer_shape x aspect_ratios), last_dimension)\n","            ssd300 conv9_2 shape => (5 x 5 x 6) = 150\n","        conv10_2 = (batch_size, (layer_shape x aspect_ratios), last_dimension)\n","            ssd300 conv10_2 shape => (3 x 3 x 4) = 36\n","        conv11_2 = (batch_size, (layer_shape x aspect_ratios), last_dimension)\n","            ssd300 conv11_2 shape => (1 x 1 x 4) = 4\n","                                           Total = 8732 default box\n","    outputs:\n","        merged_head = (batch_size, total_prior_boxes, last_dimension)\n","    \"\"\"\n","\n","    def __init__(self, last_dimension, **kwargs):\n","        super(HeadWrapper, self).__init__(**kwargs)\n","        self.last_dimension = last_dimension\n","\n","    def get_config(self):\n","        config = super(HeadWrapper, self).get_config()\n","        config.update({\"last_dimension\": self.last_dimension})\n","        return config\n","\n","    def call(self, inputs):\n","        last_dimension = self.last_dimension\n","        batch_size = tf.shape(inputs[0])[0]\n","        outputs = []\n","        for conv_layer in inputs:\n","            outputs.append(tf.reshape(conv_layer, (batch_size, -1, last_dimension)))\n","        #\n","        return tf.concat(outputs, axis=1)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"vg-e3km3rMlP","executionInfo":{"status":"ok","timestamp":1623743833049,"user_tz":-540,"elapsed":14,"user":{"displayName":"노홍민","photoUrl":"","userId":"17395953209726241917"}}},"source":["def get_head_from_outputs(hyper_params, outputs):\n","    \"\"\"Generating ssd bbox delta and label heads.\n","    inputs:\n","        hyper_params = dictionary\n","        outputs = list of ssd layers output to be used for prediction\n","    outputs:\n","        pred_deltas = merged outputs for bbox delta head\n","        pred_labels = merged outputs for bbox label head\n","    \"\"\"\n","    total_labels = hyper_params[\"n_classes\"]\n","    # +1 for ratio 1\n","    len_aspect_ratios = [len(x) + 1 for x in hyper_params[\"aspect_ratios\"]]\n","    labels_head = []\n","    boxes_head = []\n","    for i, output in enumerate(outputs):\n","        aspect_ratio = len_aspect_ratios[i]\n","        labels_head.append(Conv2D(aspect_ratio * total_labels, (3, 3), padding=\"same\", name=\"{}_conv_label_output\".format(i+1))(output))\n","        boxes_head.append(Conv2D(aspect_ratio * 4, (3, 3), padding=\"same\", name=\"{}_conv_boxes_output\".format(i+1))(output))\n","    # Classification\n","    pred_labels = HeadWrapper(total_labels, name=\"labels_head\")(labels_head)\n","    pred_labels = Activation(\"softmax\", name=\"conf\")(pred_labels)\n","    # Regression\n","    pred_deltas = HeadWrapper(4, name=\"loc\")(boxes_head)\n","    return pred_deltas, pred_labels"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"C0zmjwgxQDeV","executionInfo":{"status":"ok","timestamp":1623743833050,"user_tz":-540,"elapsed":14,"user":{"displayName":"노홍민","photoUrl":"","userId":"17395953209726241917"}}},"source":["from tensorflow.keras.layers import Layer\n","from tensorflow.keras.regularizers import l2\n","\n","class L2Normalization(Layer):\n","    \"\"\"Normalizing different scale features for fusion.\n","    paper: https://arxiv.org/abs/1506.04579\n","    inputs:\n","        feature_map = (batch_size, feature_map_height, feature_map_width, depth)\n","    outputs:\n","        normalized_feature_map = (batch_size, feature_map_height, feature_map_width, depth)\n","    \"\"\"\n","    def __init__(self, scale_factor, **kwargs):\n","        super(L2Normalization, self).__init__(**kwargs)\n","        self.scale_factor = scale_factor\n","\n","    def get_config(self):\n","        config = super(L2Normalization, self).get_config()\n","        config.update({\"scale_factor\": self.scale_factor})\n","        return config\n","\n","    def build(self, input_shape):\n","        # Network need to learn scale factor for each channel\n","        init_scale_factor = tf.fill((input_shape[-1],), float(self.scale_factor))\n","        self.scale = tf.Variable(init_scale_factor, trainable=True)\n","\n","    def call(self, inputs):\n","        return tf.nn.l2_normalize(inputs, axis=-1) * self.scale"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"hokQKN1PQG3h","executionInfo":{"status":"ok","timestamp":1623743833050,"user_tz":-540,"elapsed":13,"user":{"displayName":"노홍민","photoUrl":"","userId":"17395953209726241917"}}},"source":["## backbone : VGG\n","import tensorflow as tf\n","from tensorflow.keras.layers import Layer, Input, Conv2D, MaxPool2D\n","from tensorflow.keras.models import Model\n","\n","def get_backbone_vgg(hyper_params):\n","    \"\"\"Generating ssd model for hyper params.\n","    inputs:\n","        hyper_params = dictionary\n","    outputs:\n","        ssd_model = tf.keras.model\n","    \"\"\"\n","    # Initial scale factor 20 in the paper.\n","    # Even if this scale factor could cause loss value to be NaN in some of the cases,\n","    # it was decided to remain the same after some tests.\n","    scale_factor = 20.0 \n","    reg_factor = 5e-4\n","    n_classes = hyper_params[\"n_classes\"]\n","    # +1 for ratio 1\n","    len_aspect_ratios = [len(x) + 1 for x in hyper_params[\"aspect_ratios\"]]\n","    #\n","    input = Input(shape=(None, None, 3), name=\"input\")\n","    # conv1 block\n","    conv1_1 = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=\"glorot_normal\", kernel_regularizer=l2(reg_factor), name=\"conv1_1\")(input)\n","    conv1_2 = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=\"glorot_normal\", kernel_regularizer=l2(reg_factor), name=\"conv1_2\")(conv1_1)\n","    pool1 = MaxPool2D((2, 2), strides=(2, 2), padding=\"same\", name=\"pool1\")(conv1_2)\n","    # conv2 block\n","    conv2_1 = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=\"glorot_normal\", kernel_regularizer=l2(reg_factor), name=\"conv2_1\")(pool1)\n","    conv2_2 = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=\"glorot_normal\", kernel_regularizer=l2(reg_factor), name=\"conv2_2\")(conv2_1)\n","    pool2 = MaxPool2D((2, 2), strides=(2, 2), padding=\"same\", name=\"pool2\")(conv2_2)\n","    # conv3 block\n","    conv3_1 = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=\"glorot_normal\", kernel_regularizer=l2(reg_factor), name=\"conv3_1\")(pool2)\n","    conv3_2 = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=\"glorot_normal\", kernel_regularizer=l2(reg_factor), name=\"conv3_2\")(conv3_1)\n","    conv3_3 = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=\"glorot_normal\", kernel_regularizer=l2(reg_factor), name=\"conv3_3\")(conv3_2)\n","    pool3 = MaxPool2D((2, 2), strides=(2, 2), padding=\"same\", name=\"pool3\")(conv3_3)\n","    # conv4 block\n","    conv4_1 = Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=\"glorot_normal\", kernel_regularizer=l2(reg_factor), name=\"conv4_1\")(pool3)\n","    conv4_2 = Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=\"glorot_normal\", kernel_regularizer=l2(reg_factor), name=\"conv4_2\")(conv4_1)\n","    conv4_3 = Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=\"glorot_normal\", kernel_regularizer=l2(reg_factor), name=\"conv4_3\")(conv4_2)\n","    pool4 = MaxPool2D((2, 2), strides=(2, 2), padding=\"same\", name=\"pool4\")(conv4_3)\n","    # conv5 block\n","    conv5_1 = Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=\"glorot_normal\", kernel_regularizer=l2(reg_factor), name=\"conv5_1\")(pool4)\n","    conv5_2 = Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=\"glorot_normal\", kernel_regularizer=l2(reg_factor), name=\"conv5_2\")(conv5_1)\n","    conv5_3 = Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=\"glorot_normal\", kernel_regularizer=l2(reg_factor), name=\"conv5_3\")(conv5_2)\n","    pool5 = MaxPool2D((3, 3), strides=(1, 1), padding=\"same\", name=\"pool5\")(conv5_3)\n","    # conv6 and conv7 converted from fc6 and fc7 and remove dropouts\n","    # These layers coming from modified vgg16 model\n","    # https://gist.github.com/weiliu89/2ed6e13bfd5b57cf81d6\n","    conv6 = Conv2D(1024, (3, 3), dilation_rate=6, padding=\"same\", activation=\"relu\", kernel_initializer=\"glorot_normal\", kernel_regularizer=l2(reg_factor), name=\"conv6\")(pool5)\n","    conv7 = Conv2D(1024, (1, 1), strides=(1, 1), padding=\"same\", activation=\"relu\", kernel_initializer=\"glorot_normal\", kernel_regularizer=l2(reg_factor), name=\"conv7\")(conv6)\n","    ############################ Extra Feature Layers Start ############################\n","    # conv8 block <=> conv6 block in paper caffe implementation\n","    conv8_1 = Conv2D(256, (1, 1), strides=(1, 1), padding=\"valid\", activation=\"relu\", kernel_initializer=\"glorot_normal\", kernel_regularizer=l2(reg_factor), name=\"conv8_1\")(conv7)\n","    conv8_2 = Conv2D(512, (3, 3), strides=(2, 2), padding=\"same\", activation=\"relu\", kernel_initializer=\"glorot_normal\", kernel_regularizer=l2(reg_factor), name=\"conv8_2\")(conv8_1)\n","    # conv9 block <=> conv7 block in paper caffe implementation\n","    conv9_1 = Conv2D(128, (1, 1), strides=(1, 1), padding=\"valid\", activation=\"relu\", kernel_initializer=\"glorot_normal\", kernel_regularizer=l2(reg_factor), name=\"conv9_1\")(conv8_2)\n","    conv9_2 = Conv2D(256, (3, 3), strides=(2, 2), padding=\"same\", activation=\"relu\", kernel_initializer=\"glorot_normal\", kernel_regularizer=l2(reg_factor), name=\"conv9_2\")(conv9_1)\n","    # conv10 block <=> conv8 block in paper caffe implementation\n","    conv10_1 = Conv2D(128, (1, 1), strides=(1, 1), padding=\"valid\", activation=\"relu\", kernel_initializer=\"glorot_normal\", kernel_regularizer=l2(reg_factor), name=\"conv10_1\")(conv9_2)\n","    conv10_2 = Conv2D(256, (3, 3), strides=(1, 1), padding=\"valid\", activation=\"relu\", kernel_initializer=\"glorot_normal\", kernel_regularizer=l2(reg_factor), name=\"conv10_2\")(conv10_1)\n","    # conv11 block <=> conv9 block in paper caffe implementation\n","    conv11_1 = Conv2D(128, (1, 1), strides=(1, 1), padding=\"valid\", activation=\"relu\", kernel_initializer=\"glorot_normal\", kernel_regularizer=l2(reg_factor), name=\"conv11_1\")(conv10_2)\n","    conv11_2 = Conv2D(256, (3, 3), strides=(1, 1), padding=\"valid\", activation=\"relu\", kernel_initializer=\"glorot_normal\", kernel_regularizer=l2(reg_factor), name=\"conv11_2\")(conv11_1)\n","    ############################ Extra Feature Layers End ############################\n","    # l2 normalization for each location in the feature map\n","    conv4_3_norm = L2Normalization(scale_factor)(conv4_3)\n","    #\n","    pred_deltas, pred_labels = get_head_from_outputs(hyper_params, [conv4_3_norm, conv7, conv8_2, conv9_2, conv10_2, conv11_2])\n","    return Model(inputs=input, outputs=[pred_deltas, pred_labels])\n","\n","def init_model(model):\n","    \"\"\"Initializing model with dummy data for load weights with optimizer state and also graph construction.\n","    inputs:\n","        model = tf.keras.model\n","    \"\"\"\n","    model(tf.random.uniform((1, 512, 512, 3)))"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AblTDnuhWhqv"},"source":["### Loss Function"]},{"cell_type":"code","metadata":{"id":"iP2f9UllWiCF","executionInfo":{"status":"ok","timestamp":1623744122369,"user_tz":-540,"elapsed":389,"user":{"displayName":"노홍민","photoUrl":"","userId":"17395953209726241917"}}},"source":["import tensorflow as tf\n","\n","class CustomLoss(object):\n","    def __init__(self, neg_pos_ratio, loc_loss_alpha):\n","        self.neg_pos_ratio = tf.constant(neg_pos_ratio, dtype=tf.float32)\n","        self.loc_loss_alpha = tf.constant(loc_loss_alpha, dtype=tf.float32)\n","\n","    def loc_loss_fn(self, actual_deltas, pred_deltas):\n","        \"\"\"Calculating SSD localization loss value for only positive samples.\n","        inputs:\n","            actual_deltas = (batch_size, total_prior_boxes, [delta_y, delta_x, delta_h, delta_w])\n","            pred_deltas = (batch_size, total_prior_boxes, [delta_y, delta_x, delta_h, delta_w])\n","        outputs:\n","            loc_loss = localization / regression / bounding box loss value\n","        \"\"\"\n","        # Localization / bbox / regression loss calculation for all bboxes\n","        loc_loss_fn = tf.losses.Huber(reduction=tf.losses.Reduction.NONE)\n","        loc_loss_for_all = loc_loss_fn(actual_deltas, pred_deltas)\n","        # After tf 2.2.0 version, the huber calculates mean over the last axis\n","        loc_loss_for_all = tf.cond(tf.greater(tf.rank(loc_loss_for_all), tf.constant(2)),\n","                                   lambda: tf.reduce_sum(loc_loss_for_all, axis=-1),\n","                                   lambda: loc_loss_for_all * tf.cast(tf.shape(pred_deltas)[-1], dtype=tf.float32))\n","        #\n","        pos_cond = tf.reduce_any(tf.not_equal(actual_deltas, tf.constant(0.0)), axis=2)\n","        pos_mask = tf.cast(pos_cond, dtype=tf.float32)\n","        total_pos_bboxes = tf.reduce_sum(pos_mask, axis=1)\n","        #\n","        loc_loss = tf.reduce_sum(pos_mask * loc_loss_for_all, axis=-1)\n","        total_pos_bboxes = tf.where(tf.equal(total_pos_bboxes, tf.constant(0.0)), tf.constant(1.0), total_pos_bboxes)\n","        loc_loss = loc_loss / total_pos_bboxes\n","        #\n","        return loc_loss * self.loc_loss_alpha\n","\n","    def conf_loss_fn(self, actual_labels, pred_labels):\n","        \"\"\"Calculating SSD confidence loss value by performing hard negative mining as mentioned in the paper.\n","        inputs:\n","            actual_labels = (batch_size, total_prior_boxes, total_labels)\n","            pred_labels = (batch_size, total_prior_boxes, total_labels)\n","        outputs:\n","            conf_loss = confidence / class / label loss value\n","        \"\"\"\n","        # Confidence / Label loss calculation for all labels\n","        conf_loss_fn = tf.losses.CategoricalCrossentropy(reduction=tf.losses.Reduction.NONE)\n","        conf_loss_for_all = conf_loss_fn(actual_labels, pred_labels)\n","        #\n","        pos_cond = tf.reduce_any(tf.not_equal(actual_labels[..., 1:], tf.constant(0.0)), axis=2)\n","        pos_mask = tf.cast(pos_cond, dtype=tf.float32)\n","        total_pos_bboxes = tf.reduce_sum(pos_mask, axis=1)\n","        # Hard negative mining\n","        total_neg_bboxes = tf.cast(total_pos_bboxes * self.neg_pos_ratio, tf.int32)\n","        #\n","        masked_loss = conf_loss_for_all * actual_labels[..., 0]\n","        sorted_loss = tf.argsort(masked_loss, direction=\"DESCENDING\")\n","        sorted_loss = tf.argsort(sorted_loss)\n","        neg_cond = tf.less(sorted_loss, tf.expand_dims(total_neg_bboxes, axis=1))\n","        neg_mask = tf.cast(neg_cond, dtype=tf.float32)\n","        #\n","        final_mask = pos_mask + neg_mask\n","        conf_loss = tf.reduce_sum(final_mask * conf_loss_for_all, axis=-1)\n","        total_pos_bboxes = tf.where(tf.equal(total_pos_bboxes, tf.constant(0.0)), tf.constant(1.0), total_pos_bboxes)\n","        conf_loss = conf_loss / total_pos_bboxes\n","        #\n","        return conf_loss"],"execution_count":40,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BuzyIoJdUXXP"},"source":["### Training"]},{"cell_type":"code","metadata":{"id":"ExUO_s9lcPxP","executionInfo":{"status":"ok","timestamp":1623744125111,"user_tz":-540,"elapsed":378,"user":{"displayName":"노홍민","photoUrl":"","userId":"17395953209726241917"}}},"source":["import math\n","def get_step_size(data_length, batch_size):\n","    \"\"\"Get step size for given total item size and batch size.\n","    inputs:\n","        total_items = number of total items\n","        batch_size = number of batch size during training or validation\n","    outputs:\n","        step_size = number of step size for model training\n","    \"\"\"\n","    return math.ceil(data_length / batch_size)"],"execution_count":41,"outputs":[]},{"cell_type":"code","metadata":{"id":"VMt4zJZPV2Ez","executionInfo":{"status":"ok","timestamp":1623744126131,"user_tz":-540,"elapsed":7,"user":{"displayName":"노홍민","photoUrl":"","userId":"17395953209726241917"}}},"source":["import os\n","import time\n","import tensorflow as tf\n","\n","# hyper_params config\n","hyper_params = {}\n","hyper_params[\"image_size\"] = (300, 300)\n","hyper_params[\"aspect_ratios\"] = [[1.0, 2.0, 0.5],\n","                                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n","                                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n","                                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n","                                 [1.0, 2.0, 0.5],\n","                                 [1.0, 2.0, 0.5]]\n","hyper_params[\"feature_map_shapes\"] = [38, 19, 10, 5, 3, 1]\n","hyper_params[\"n_classes\"] = 11+1\n","hyper_params[\"iou_threshold\"] = 0.5\n","hyper_params[\"neg_pos_ratio\"] = 3\n","hyper_params[\"loc_loss_alpha\"] = 1\n","hyper_params[\"variances\"] = [0.1, 0.1, 0.2, 0.2]\n","# hyper_params[\"scales\"] = [0.1, 0.2, 0.37, 0.54, 0.71, 0.88, 1.05]\n","# hyper_params[\"mean_color\"] = [123, 117, 104]"],"execution_count":42,"outputs":[]},{"cell_type":"code","metadata":{"id":"bvT8MZpTQiNE","executionInfo":{"status":"ok","timestamp":1623744127384,"user_tz":-540,"elapsed":3,"user":{"displayName":"노홍민","photoUrl":"","userId":"17395953209726241917"}}},"source":["data_dir = os.path.join(os.getcwd(), 'dataset')\n","IMAGE_SIZE = (300, 300)\n","BATCH_SIZE = 32\n","EPOCHS = 2"],"execution_count":43,"outputs":[]},{"cell_type":"code","metadata":{"id":"qMTARWUlQdTG","executionInfo":{"status":"ok","timestamp":1623744128353,"user_tz":-540,"elapsed":4,"user":{"displayName":"노홍민","photoUrl":"","userId":"17395953209726241917"}}},"source":["data_shapes = get_data_shapes()\n","padding_values = get_padding_values()\n","\n","train_ds, train_len = ds_generator('train.csv', image_size=IMAGE_SIZE)\n","val_ds, val_len = ds_generator('validation.csv', image_size=IMAGE_SIZE)\n","train_ds = train_ds.shuffle(100).padded_batch(BATCH_SIZE, padded_shapes=data_shapes, padding_values=padding_values)\n","val_ds = val_ds.padded_batch(BATCH_SIZE, padded_shapes=data_shapes, padding_values=padding_values)"],"execution_count":44,"outputs":[]},{"cell_type":"code","metadata":{"id":"AyypaJuwYasy","executionInfo":{"status":"ok","timestamp":1623744129538,"user_tz":-540,"elapsed":421,"user":{"displayName":"노홍민","photoUrl":"","userId":"17395953209726241917"}}},"source":["from tensorflow.keras.optimizers import SGD, Adam\n","\n","ssd_model = get_backbone_vgg(hyper_params)\n","ssd_custom_losses = CustomLoss(hyper_params[\"neg_pos_ratio\"], hyper_params[\"loc_loss_alpha\"])\n","ssd_model.compile(optimizer=Adam(learning_rate=1e-3),\n","                  loss=[ssd_custom_losses.loc_loss_fn, ssd_custom_losses.conf_loss_fn])\n","init_model(ssd_model)"],"execution_count":45,"outputs":[]},{"cell_type":"code","metadata":{"id":"ATOyXZrYZ7V0","executionInfo":{"status":"ok","timestamp":1623744129539,"user_tz":-540,"elapsed":5,"user":{"displayName":"노홍민","photoUrl":"","userId":"17395953209726241917"}}},"source":["# We calculate prior boxes for one time and use it for all operations because of the all images are the same sizes\n","prior_boxes = generate_prior_boxes(hyper_params[\"feature_map_shapes\"], hyper_params[\"aspect_ratios\"])\n","ssd_train_feed = generator(train_ds, prior_boxes, hyper_params)\n","ssd_val_feed = generator(val_ds, prior_boxes, hyper_params)"],"execution_count":46,"outputs":[]},{"cell_type":"code","metadata":{"id":"UzeC2lOqYRVS","executionInfo":{"status":"ok","timestamp":1623744130533,"user_tz":-540,"elapsed":3,"user":{"displayName":"노홍민","photoUrl":"","userId":"17395953209726241917"}}},"source":["def scheduler(epoch):\n","    \"\"\"Generating learning rate value for a given epoch.\n","    inputs:\n","        epoch = number of current epoch\n","    outputs:\n","        learning_rate = float learning rate value\n","    \"\"\"\n","    if epoch < 100:\n","        return 1e-3\n","    elif epoch < 125:\n","        return 1e-4\n","    else:\n","        return 1e-5"],"execution_count":47,"outputs":[]},{"cell_type":"code","metadata":{"id":"sj7Zs9T1Z9Kc","executionInfo":{"status":"ok","timestamp":1623744132049,"user_tz":-540,"elapsed":776,"user":{"displayName":"노홍민","photoUrl":"","userId":"17395953209726241917"}}},"source":["from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, LearningRateScheduler\n","from datetime import datetime\n","ssd_log_path = os.path.join(\"logs\", \"VGG\", datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","# ssd_log_path = \"logs/{}{}/{}\".format(BACKBONE, datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","ssd_model_path = os.path.join(\"trained\", datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","model_name = os.path.join(ssd_model_path, \"ssd300_vgg_weights.h5\")\n","\n","checkpoint_callback = ModelCheckpoint(model_name, monitor=\"val_loss\", save_best_only=True, save_weights_only=True)\n","tensorboard_callback = TensorBoard(log_dir=ssd_log_path)\n","learning_rate_callback = LearningRateScheduler(scheduler, verbose=0)"],"execution_count":48,"outputs":[]},{"cell_type":"code","metadata":{"id":"8VB_fPAkjUtV","colab":{"base_uri":"https://localhost:8080/","height":412},"executionInfo":{"status":"error","timestamp":1623744236604,"user_tz":-540,"elapsed":103835,"user":{"displayName":"노홍민","photoUrl":"","userId":"17395953209726241917"}},"outputId":"591b48e2-5577-47a8-80d9-d0beeb012ee1"},"source":["step_size_train = get_step_size(train_len, BATCH_SIZE)\n","step_size_val = get_step_size(val_len, BATCH_SIZE)\n","\n","os.makedirs(ssd_model_path, exist_ok=True)\n","os.makedirs(ssd_log_path, exist_ok=True)\n","\n","ssd_model.fit(ssd_train_feed,\n","              steps_per_epoch=step_size_train,\n","              validation_data=ssd_val_feed,\n","              validation_steps=step_size_val,\n","              epochs=EPOCHS,\n","              callbacks=[checkpoint_callback, tensorboard_callback, learning_rate_callback]\n","              )"],"execution_count":49,"outputs":[{"output_type":"stream","text":["Epoch 1/2\n","36/95 [==========>...................] - ETA: 53s - loss: 545.0338 - loc_loss: 84.9031 - conf_loss: 457.1642"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-49-7badd37471a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m               \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep_size_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorboard_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m               )\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}